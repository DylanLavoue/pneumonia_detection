{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We're running localy\n"
     ]
    }
   ],
   "source": [
    "# Import base librairies\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "# Import scientific librairies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import Tensorflow and Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "# Import scikit-learn\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "# Check running environment\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    IN_COLAB=True\n",
    "except:\n",
    "    IN_COLAB=False\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"We're running on Colab\")\n",
    "    \n",
    "    # Mount google drive\n",
    "    mounting_point = \"/content/drive/\"\n",
    "    drive.mount(mounting_point, force_remount=True)\n",
    "\n",
    "    # Add project directory to kernel paths\n",
    "    drive_folder = \"MyDrive/pneumonia_detection/T-DEV-810-PAR_10\"\n",
    "    \n",
    "    sys.path.append(mounting_point + drive_folder)\n",
    "else:\n",
    "    print(\"We're running localy\")\n",
    "    \n",
    "    # Add project directory to kernel paths\n",
    "    sys.path.append('../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected distribution strategy:                     _DefaultDistributionStrategy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-14 19:47:49.092149: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:266] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    }
   ],
   "source": [
    "# Import custom functions\n",
    "from src.data.file_manager import FileManager\n",
    "from src.data.tf_utils import load_image_dataset_from_tfrecord, define_distribute_strategy\n",
    "from src.data.evaluation import Evaluation\n",
    "\n",
    "zoidbergManager = FileManager()\n",
    "strategy = define_distribute_strategy()\n",
    "evaluation = Evaluation(strategy)\n",
    "\n",
    "# Set default graphics visualization\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed for keras, numpy, tensorflow, and the 'random' module\n",
    "SEED = 42\n",
    "tf.keras.utils.set_random_seed(SEED)\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***2. Loading dataset*** *texte en italique*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why have a batch size of 64? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "SMALL_TRAIN_SPLIT = 0.2\n",
    "SMALL_VAL_SPLIT = 0.15\n",
    "class_names = ['bacteria', 'normal', 'virus']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load the train and val datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dylan/Desktop/Pneumonia/pneumonia_detection/pneumonia/notebooks/models/../../data/processed\n"
     ]
    }
   ],
   "source": [
    "processed_dir_path = zoidbergManager.data_dir / 'processed'\n",
    "print(processed_dir_path)\n",
    "\n",
    "train_path = str(processed_dir_path / 'train_512x512_rgb_ds.tfrecord')\n",
    "val_path = str(processed_dir_path / 'val_512x512_rgb_ds.tfrecord')\n",
    "\n",
    "train_ds = load_image_dataset_from_tfrecord(train_path)\n",
    "val_ds = load_image_dataset_from_tfrecord(val_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we extract a small part of each datasets to train each models on a small dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-14 19:47:49.559445: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [1]\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2023-05-14 19:47:51.333153: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [1]\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2023-05-14 19:47:51.484974: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [1]\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2023-05-14 19:47:51.485130: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [1]\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In training dataset, there are :\n",
      "  - 453 files for class bacteria\n",
      "  - 237 files for class normal\n",
      "  - 247 files for class virus\n",
      "\n",
      "In val dataset, there are :\n",
      "  - 20 files for class bacteria\n",
      "  - 11 files for class normal\n",
      "  - 12 files for class virus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-14 19:47:53.826281: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [1]\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2023-05-14 19:47:53.826508: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [1]\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    }
   ],
   "source": [
    "num_train_img = train_ds.reduce(0, lambda x, _: x + 1).numpy()\n",
    "num_val_img = val_ds.reduce(0, lambda x, _: x + 1).numpy()\n",
    "\n",
    "# Shuffle data\n",
    "train_ds = train_ds.shuffle(buffer_size=num_train_img, seed=42)\n",
    "val_ds = val_ds.shuffle(buffer_size=num_val_img, seed=42)\n",
    "\n",
    "# Extract a sample\n",
    "small_train_size = int(num_train_img * SMALL_TRAIN_SPLIT)\n",
    "small_val_size = int(num_val_img * SMALL_VAL_SPLIT)\n",
    "\n",
    "small_train_ds = train_ds.take(small_train_size)\n",
    "small_val_ds = val_ds.take(small_val_size)\n",
    "\n",
    "def count_img_by_class(dataset, class_names=class_names):\n",
    "    num_img_by_classes = {name:0 for name in class_names}\n",
    "    for images, labels in dataset:\n",
    "        idx_label = np.nonzero(labels.numpy())[0][0]\n",
    "        for idx, name in enumerate(class_names):\n",
    "            if idx_label == idx:\n",
    "                num_img_by_classes[name] += 1\n",
    "    return num_img_by_classes\n",
    "\n",
    "print(\"In training dataset, there are :\")\n",
    "for class_name, num_img in count_img_by_class(small_train_ds).items():\n",
    "    print(f\"  - {num_img} files for class {class_name}\")    \n",
    "print(\"\\nIn val dataset, there are :\")\n",
    "for class_name, num_img in count_img_by_class(small_val_ds).items():\n",
    "    print(f\"  - {num_img} files for class {class_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch & prefecth data to improve computation time\n",
    "small_train_ds = small_train_ds.batch(BATCH_SIZE).prefetch(\n",
    "    buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "small_val_ds = small_val_ds.batch(BATCH_SIZE).prefetch(\n",
    "    buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Compare a bunch of bunch of models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now train models on this small dataset. We set useful variables below.\n",
    "\n",
    "⚠️⚠️⚠️ WARNING : Depending on your hardware, training cells can be computationally expensive and take a really long time to run them !!!\n",
    "That's why each of these cells are wrapped in a if condition (see TRAIN_*MODEL* booleans below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 30\n",
    "LEARNING_RATE = 0.0001\n",
    "\n",
    "TRAIN_VGG16 = False\n",
    "TRAIN_RESNET50 = False\n",
    "TRAIN_INCEPTION_RESNET = False\n",
    "TRAIN_XCEPTION = False\n",
    "TRAIN_EFFICIENTNETB0 = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define also 2 callbacks :\n",
    "\n",
    "checkpoint_cb : save model at each epoch (only save best weight).\n",
    "earlystopping_cb : stop training if model does not progress. It is faster and helps against overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def checkpoint_cb(model):\n",
    "    checkpoint_dir = zoidbergManager.model_dir / 'checkpoints'\n",
    "    checkpoint_filepath = checkpoint_dir / f'ckpt_smallds_{model.name}.h5'\n",
    "    ckpt_cb = keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_filepath,\n",
    "        monitor='val_MCC',\n",
    "        mode='max',\n",
    "        save_best_only=True\n",
    "    )\n",
    "    return ckpt_cb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we compute class weights to prevent imbalanced classes (as we saw when we analyzed data) :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-14 19:47:54.115444: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [1]\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2023-05-14 19:47:54.115710: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [1]\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class bacteria => weight : 0.693458\n",
      "class normal => weight : 1.248335\n",
      "class virus => weight : 1.321207\n"
     ]
    }
   ],
   "source": [
    "y_train_iterator = train_ds.map(lambda x, y: y).as_numpy_iterator()\n",
    "y_train = []\n",
    "for one_vector in y_train_iterator:\n",
    "    y_train.append(one_vector)\n",
    "y_train = np.argmax(y_train, axis=1)\n",
    "\n",
    "class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights\n",
    "dic_class_weights = {}\n",
    "for idx, weight in enumerate(class_weights):\n",
    "    dic_class_weights[idx] = weight\n",
    "    print(f'class {class_names[idx]} => weight : {weight:2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_history(model, history, training_time):\n",
    "    dic = {}\n",
    "    dic['history'] = history.history\n",
    "    dic['training_time'] = training_time\n",
    "    dic['epoch_time'] = training_time/len(history.history['loss'])\n",
    "    history_dir = zoidbergManager.model_dir / 'histories'\n",
    "    history_filepath = history_dir / f'hty_smallds_{model.name}.json'\n",
    "    json.dump(dic, open(history_filepath, 'w'))\n",
    "\n",
    "def train_model(model, save=False):\n",
    "    start_time = time.time()\n",
    "    history = model.fit(small_train_ds,\n",
    "                        validation_data=small_val_ds,\n",
    "                        epochs=EPOCHS,\n",
    "                        #steps_per_epoch=(small_train_size // BATCH_SIZE + 1),\n",
    "                        class_weight=dic_class_weights,\n",
    "                        callbacks=[checkpoint_cb(model)],\n",
    "                        )\n",
    "    training_time = time.time() - start_time\n",
    "\n",
    "    if save:\n",
    "        save_history(model, history, training_time)\n",
    "\n",
    "    return history, training_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We selected 4 kinds of models to try. Those models have been chosen base on their performance :\n",
    "\n",
    "VGG and ResNet have been used several times on this dataset with some good performances. Many notebooks can be found on kaggle\n",
    "Xception and EfficientNet have good results on imagenet with few parameters.\n",
    "One can check performance of each model on imagenet here : keras.applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1 VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vgg16():\n",
    "    base_vgg16 = tf.keras.applications.VGG16(weights='imagenet', input_shape=(224,224,3), include_top=False)\n",
    "    for layer in base_vgg16.layers:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    vgg16 = tf.keras.Sequential([\n",
    "        keras.layers.InputLayer(input_shape=(512,512,3), name='input'),\n",
    "        keras.layers.Resizing(224, 224, interpolation=\"bilinear\", name='resize'),\n",
    "        keras.layers.Rescaling(scale=1./255., name='rescale'),\n",
    "        base_vgg16,\n",
    "        keras.layers.Flatten(name='flatten'),\n",
    "        keras.layers.Dense(1024, activation='relu', name='fully_conn1'),\n",
    "        keras.layers.Dense(512, activation='relu', name='fully_conn2'),\n",
    "        keras.layers.Dense(3, activation='softmax', name='out_softmax'),\n",
    "    ], name = 'vgg16')\n",
    "\n",
    "    vgg16.compile(optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=evaluation.get_training_metrics()\n",
    "                 )\n",
    "    return vgg16\n",
    "    \n",
    "with strategy.scope():\n",
    "    vgg16 = make_vgg16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_VGG16:\n",
    "    vgg16_history, vgg16_time = train_model(vgg16, save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2 ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_resnet50():\n",
    "    base_resnet50 = tf.keras.applications.ResNet50V2(weights='imagenet', input_shape=(224,224,3), include_top=False)\n",
    "    for layer in base_resnet50.layers:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    resnet50 = tf.keras.Sequential([\n",
    "        keras.layers.InputLayer(input_shape=(512,512,3), name='input'),\n",
    "        keras.layers.Resizing(224, 224, interpolation=\"bilinear\", name='resize'),\n",
    "        keras.layers.Rescaling(scale=1./255., name='rescale'),\n",
    "        base_resnet50,\n",
    "        keras.layers.GlobalAveragePooling2D(name='avg_pool'),\n",
    "        keras.layers.Dense(1024, activation='relu', name='fully_conn1'),\n",
    "        keras.layers.Dense(512, activation='relu', name='fully_conn2'),\n",
    "        keras.layers.Dense(3, activation='softmax', name='out_softmax'),\n",
    "    ], name = 'resnet50')\n",
    "\n",
    "    resnet50.compile(optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=evaluation.get_training_metrics()\n",
    "                 )\n",
    "    return resnet50\n",
    "    \n",
    "with strategy.scope():\n",
    "    resnet50 = make_resnet50()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_RESNET50:\n",
    "    resnet50_history, resnet50_time = train_model(resnet50, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_inception_resnet():\n",
    "    base_inception_resnet = tf.keras.applications.InceptionResNetV2(weights='imagenet', input_shape=(299,299,3), include_top=False)\n",
    "    for layer in base_inception_resnet.layers:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    inception_resnet = tf.keras.Sequential([\n",
    "        keras.layers.InputLayer(input_shape=(512,512,3), name='input'),\n",
    "        keras.layers.Resizing(299, 299, interpolation=\"bilinear\", name='resize'),\n",
    "        keras.layers.Rescaling(scale=1./255., name='rescale'),\n",
    "        base_inception_resnet,\n",
    "        keras.layers.GlobalAveragePooling2D(name='avg_pool'),\n",
    "        keras.layers.Dense(1024, activation='relu', name='fully_conn1'),\n",
    "        keras.layers.Dense(512, activation='relu', name='fully_conn2'),\n",
    "        keras.layers.Dense(3, activation='softmax', name='out_softmax'),\n",
    "    ], name = 'inception_resnet')\n",
    "\n",
    "    inception_resnet.compile(optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=evaluation.get_training_metrics()\n",
    "                 )\n",
    "    return inception_resnet\n",
    "    \n",
    "with strategy.scope():\n",
    "    inception_resnet = make_inception_resnet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_INCEPTION_RESNET:\n",
    "    inception_resnet_history, inception_resnet_time = train_model(inception_resnet, save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.4 EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_efficientnetb0():\n",
    "    base_efficientnetb0 = tf.keras.applications.EfficientNetV2B0(weights='imagenet', input_shape=(224,224,3), include_top=False)\n",
    "    for layer in base_efficientnetb0.layers:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    efficientnetb0 = tf.keras.Sequential([\n",
    "        keras.layers.InputLayer(input_shape=(512,512,3), name='input'),\n",
    "        keras.layers.Resizing(224, 224, interpolation=\"bilinear\", name='resize'),\n",
    "        base_efficientnetb0,\n",
    "        keras.layers.GlobalAveragePooling2D(name='avg_pool'),\n",
    "        keras.layers.Dense(1024, activation='relu', name='fully_conn1'),\n",
    "        keras.layers.Dense(512, activation='relu', name='fully_conn2'),\n",
    "        keras.layers.Dense(3, activation='softmax', name='out_softmax'),\n",
    "    ], name = 'efficientnetb0')\n",
    "\n",
    "    efficientnetb0.compile(optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=evaluation.get_training_metrics()\n",
    "                 )\n",
    "    return efficientnetb0\n",
    "    \n",
    "with strategy.scope():\n",
    "    efficientnetb0 = make_efficientnetb0()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_EFFICIENTNETB0:\n",
    "    efficientnetb0_history, efficientnetb0_time = train_model(efficientnetb0, save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.5 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/dylan/Desktop/Pneumonia/pneumonia_detection/pneumonia/notebooks/models/../../models/histories/hty_smallds_vgg16.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m histories \u001b[39m=\u001b[39m {}\n\u001b[1;32m     19\u001b[0m \u001b[39mfor\u001b[39;00m model_name, path \u001b[39min\u001b[39;00m history_path\u001b[39m.\u001b[39mitems():\n\u001b[0;32m---> 20\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(path) \u001b[39mas\u001b[39;00m file:\n\u001b[1;32m     21\u001b[0m         history \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(file)\n\u001b[1;32m     22\u001b[0m     histories[model_name] \u001b[39m=\u001b[39m history[\u001b[39m'\u001b[39m\u001b[39mhistory\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[1;32m    276\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    277\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    279\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m     )\n\u001b[0;32m--> 282\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/dylan/Desktop/Pneumonia/pneumonia_detection/pneumonia/notebooks/models/../../models/histories/hty_smallds_vgg16.json'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "history_path = {\n",
    "    'vgg16' : zoidbergManager.model_dir / 'histories' / 'hty_smallds_vgg16.json',\n",
    "    'resnet50' : zoidbergManager.model_dir / 'histories' / 'hty_smallds_resnet50.json',\n",
    "    'inception_resnet' : zoidbergManager.model_dir / 'histories' / 'hty_smallds_inception_resnet.json',\n",
    "    'xception' : zoidbergManager.model_dir / 'histories' / 'hty_smallds_xception.json',\n",
    "    'efficientnetb0' : zoidbergManager.model_dir / 'histories' / 'hty_smallds_efficientnetb0.json'\n",
    "}\n",
    "\n",
    "checkpoint_path = {\n",
    "    'vgg16' : zoidbergManager.model_dir / 'checkpoints' / 'ckpt_smallds_vgg16.h5',\n",
    "    'resnet50' : zoidbergManager.model_dir / 'checkpoints' / 'ckpt_smallds_resnet50.h5',\n",
    "    'inception_resnet' : zoidbergManager.model_dir / 'checkpoints' / 'ckpt_smallds_inception_resnet.h5',\n",
    "    'xception' : zoidbergManager.model_dir / 'checkpoints' / 'ckpt_smallds_xception.h5',\n",
    "    'efficientnetb0' : zoidbergManager.model_dir / 'checkpoints' / 'ckpt_smallds_efficientnetb0.h5'\n",
    "}\n",
    "\n",
    "sumup_result_df = pd.DataFrame(columns=['model','size','training_time','max MCC','max val_MCC'])\n",
    "histories = {}\n",
    "for model_name, path in history_path.items():\n",
    "    with open(path) as file:\n",
    "        history = json.load(file)\n",
    "    histories[model_name] = history['history']\n",
    "    sumup = []\n",
    "    sumup.append(model_name)\n",
    "    sumup.append(f'{checkpoint_path[model_name].stat().st_size / (1e6):.1f} MB')\n",
    "    sumup.append(f\"{history['training_time']:.3f} s\")\n",
    "    sumup.append(f\"{np.max(history['history']['MCC']):.3f}\")\n",
    "    sumup.append(f\"{np.max(history['history']['val_MCC']):.3f}\")\n",
    "    idx_model = list(history_path.keys()).index(model_name)\n",
    "    sumup_result_df.loc[idx_model] = sumup\n",
    "\n",
    "sumup_result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcc_history = {}\n",
    "for model, history in histories.items():\n",
    "    mcc_history[model] = history['val_MCC']\n",
    "\n",
    "sns.set_style('ticks')\n",
    "bp = sns.boxplot(data=pd.DataFrame.from_dict(mcc_history).iloc[-5:])\n",
    "bp.set_title('val_MCC over last 5 epochs')\n",
    "sns.despine(offset=10, trim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = list(range(1,EPOCHS+1))\n",
    "full_mcc_history = []\n",
    "for model, history in histories.items():\n",
    "    data = {}\n",
    "    data['epoch'] = epochs\n",
    "    data['model'] = [model]*EPOCHS\n",
    "    data['MCC'] = history['MCC']\n",
    "    data['val_MCC'] = history['val_MCC']\n",
    "    full_mcc_history.append(pd.DataFrame.from_dict(data))\n",
    "full_mcc_history_df = pd.concat(full_mcc_history)\n",
    "\n",
    "sns.set_style('ticks')\n",
    "rp = sns.relplot(\n",
    "    data=pd.melt(full_mcc_history_df,['epoch','model']),\n",
    "    x=\"epoch\",\n",
    "    y=\"value\",\n",
    "    hue=\"variable\",\n",
    "    col=\"model\",\n",
    "    col_wrap=2,\n",
    "    height=2.5,\n",
    "    aspect=2.2,\n",
    "    kind=\"line\"\n",
    ")\n",
    "rp.fig.suptitle('Learning curves', fontsize=14)\n",
    "rp.fig.subplots_adjust(top=0.9)\n",
    "sns.move_legend(rp,\n",
    "                'center',\n",
    "                bbox_to_anchor=(.7, .18),\n",
    "                fontsize=14,\n",
    "                title_fontsize=14,\n",
    "                frameon=True,\n",
    "                borderpad=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
