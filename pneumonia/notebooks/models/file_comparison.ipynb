{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We're running localy\n"
     ]
    }
   ],
   "source": [
    "# Import base librairies\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "# Import scientific librairies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import Tensorflow and Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "# Import scikit-learn\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "# Check running environment\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    IN_COLAB=True\n",
    "except:\n",
    "    IN_COLAB=False\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"We're running on Colab\")\n",
    "    \n",
    "    # Mount google drive\n",
    "    mounting_point = \"/content/drive/\"\n",
    "    drive.mount(mounting_point, force_remount=True)\n",
    "\n",
    "    # Add project directory to kernel paths\n",
    "    drive_folder = \"MyDrive/pneumonia_detection/T-DEV-810-PAR_10\"\n",
    "    \n",
    "    sys.path.append(mounting_point + drive_folder)\n",
    "else:\n",
    "    print(\"We're running localy\")\n",
    "    \n",
    "    # Add project directory to kernel paths\n",
    "    sys.path.append('../..')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected distribution strategy:                     _DefaultDistributionStrategy\n"
     ]
    }
   ],
   "source": [
    "# Import custom functions\n",
    "from src.data.file_manager import FileManager\n",
    "from src.data.tf_utils import load_image_dataset_from_tfrecord, define_distribute_strategy\n",
    "from src.data.evaluation import Evaluation\n",
    "\n",
    "zoidbergManager = FileManager()\n",
    "strategy = define_distribute_strategy()\n",
    "evaluation = Evaluation(strategy)\n",
    "\n",
    "# Set default graphics visualization\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed for keras, numpy, tensorflow, and the 'random' module\n",
    "SEED = 42\n",
    "tf.keras.utils.set_random_seed(SEED)\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***2. Loading dataset*** *texte en italique*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "SMALL_TRAIN_SPLIT = 0.2\n",
    "SMALL_VAL_SPLIT = 0.15\n",
    "class_names = ['batceria', 'normal', 'virus']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load the train and val datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dir_path = zoidbergManager.data_dir / 'processed'\n",
    "\n",
    "train_path = str(processed_dir_path / 'train_512x512_rgb_ds.tfrecord')\n",
    "val_path = str(processed_dir_path / 'val_512x512_rgb_ds.tfrecord')\n",
    "\n",
    "train_ds = load_image_dataset_from_tfrecord(train_path)\n",
    "val_ds = load_image_dataset_from_tfrecord(val_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we extract a small part of each datasets to train each models on a small dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_img = train_ds.reduce(0, lambda x, _: x + 1).numpy()\n",
    "num_val_img = val_ds.reduce(0, lambda x, _: x + 1).numpy()\n",
    "\n",
    "# Shuffle data\n",
    "train_ds = train_ds.shuffle(buffer_size=num_train_img, seed=42)\n",
    "val_ds = val_ds.shuffle(buffer_size=num_val_img, seed=42)\n",
    "\n",
    "# Extract a sample\n",
    "small_train_size = int(num_train_img * SMALL_TRAIN_SPLIT)\n",
    "small_val_size = int(num_val_img * SMALL_VAL_SPLIT)\n",
    "\n",
    "small_train_ds = train_ds.take(small_train_size)\n",
    "small_val_ds = val_ds.take(small_val_size)\n",
    "\n",
    "def count_img_by_class(dataset, class_names=class_names):\n",
    "    num_img_by_classes = {name:0 for name in class_names}\n",
    "    for images, labels in dataset:\n",
    "        idx_label = np.nonzero(labels.numpy())[0][0]\n",
    "        for idx, name in enumerate(class_names):\n",
    "            if idx_label == idx:\n",
    "                num_img_by_classes[name] += 1\n",
    "    return num_img_by_classes\n",
    "\n",
    "print(\"In training dataset, there are :\")\n",
    "for class_name, num_img in count_img_by_class(small_train_ds).items():\n",
    "    print(f\"  - {num_img} files for class {class_name}\")    \n",
    "print(\"\\nIn val dataset, there are :\")\n",
    "for class_name, num_img in count_img_by_class(small_val_ds).items():\n",
    "    print(f\"  - {num_img} files for class {class_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'small_train_ds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Batch & prefecth data to improve computation time\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m small_train_ds \u001b[39m=\u001b[39m small_train_ds\u001b[39m.\u001b[39mbatch(BATCH_SIZE)\u001b[39m.\u001b[39mprefetch(\n\u001b[1;32m      3\u001b[0m     buffer_size\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mAUTOTUNE)\n\u001b[1;32m      5\u001b[0m small_val_ds \u001b[39m=\u001b[39m small_val_ds\u001b[39m.\u001b[39mbatch(BATCH_SIZE)\u001b[39m.\u001b[39mprefetch(\n\u001b[1;32m      6\u001b[0m     buffer_size\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mAUTOTUNE)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'small_train_ds' is not defined"
     ]
    }
   ],
   "source": [
    "# Batch & prefecth data to improve computation time\n",
    "small_train_ds = small_train_ds.batch(BATCH_SIZE).prefetch(\n",
    "    buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "small_val_ds = small_val_ds.batch(BATCH_SIZE).prefetch(\n",
    "    buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Compare a bunch of bunch of models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now train models on this small dataset. We set useful variables below.\n",
    "\n",
    "⚠️⚠️⚠️ WARNING : Depending on your hardware, training cells can be computationally expensive and take a really long time to run them !!!\n",
    "That's why each of these cells are wrapped in a if condition (see TRAIN_*MODEL* booleans below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 30\n",
    "LEARNING_RATE = 0.0001\n",
    "\n",
    "TRAIN_VGG16 = False\n",
    "TRAIN_RESNET50 = False\n",
    "TRAIN_INCEPTION_RESNET = False\n",
    "TRAIN_XCEPTION = False\n",
    "TRAIN_EFFICIENTNETB0 = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define also 2 callbacks :\n",
    "\n",
    "checkpoint_cb : save model at each epoch (only save best weight).\n",
    "earlystopping_cb : stop training if model does not progress. It is faster and helps against overfitting."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def checkpoint_cb(model):\n",
    "    checkpoint_dir = zoidbergManager.model_dir / 'checkpoints'\n",
    "    checkpoint_filepath = checkpoint_dir / f'ckpt_smallds_{model.name}.h5'\n",
    "    ckpt_cb = keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_filepath,\n",
    "        monitor='val_MCC',\n",
    "        mode='max',\n",
    "        save_best_only=True\n",
    "    )\n",
    "    return ckpt_cb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we compute class weights to prevent imbalanced classes (as we saw when we analyzed data) :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_iterator = train_ds.map(lambda x, y: y).as_numpy_iterator()\n",
    "y_train = []\n",
    "for one_vector in y_train_iterator:\n",
    "    y_train.append(one_vector)\n",
    "y_train = np.argmax(y_train, axis=1)\n",
    "\n",
    "class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights\n",
    "dic_class_weights = {}\n",
    "for idx, weight in enumerate(class_weights):\n",
    "    dic_class_weights[idx] = weight\n",
    "    print(f'class {class_names[idx]} => weight : {weight:2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_history(model, history, training_time):\n",
    "    dic = {}\n",
    "    dic['history'] = history.history\n",
    "    dic['training_time'] = training_time\n",
    "    dic['epoch_time'] = training_time/len(history.history['loss'])\n",
    "    history_dir = zoidbergManager.model_dir / 'histories'\n",
    "    history_filepath = history_dir / f'hty_smallds_{model.name}.json'\n",
    "    json.dump(dic, open(history_filepath, 'w'))\n",
    "\n",
    "def train_model(model, save=False):\n",
    "    start_time = time.time()\n",
    "    history = model.fit(small_train_ds,\n",
    "                        validation_data=small_val_ds,\n",
    "                        epochs=EPOCHS,\n",
    "                        #steps_per_epoch=(small_train_size // BATCH_SIZE + 1),\n",
    "                        class_weight=dic_class_weights,\n",
    "                        callbacks=[checkpoint_cb(model)],\n",
    "                        )\n",
    "    training_time = time.time() - start_time\n",
    "\n",
    "    if save:\n",
    "        save_history(model, history, training_time)\n",
    "\n",
    "    return history, training_time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We selected 4 kinds of models to try. Those models have been chosen base on their performance :\n",
    "\n",
    "VGG and ResNet have been used several times on this dataset with some good performances. Many notebooks can be found on kaggle\n",
    "Xception and EfficientNet have good results on imagenet with few parameters.\n",
    "One can check performance of each model on imagenet here : keras.applications"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1 VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vgg16():\n",
    "    base_vgg16 = tf.keras.applications.VGG16(weights='imagenet', input_shape=(224,224,3), include_top=False)\n",
    "    for layer in base_vgg16.layers:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    vgg16 = tf.keras.Sequential([\n",
    "        keras.layers.InputLayer(input_shape=(512,512,3), name='input'),\n",
    "        keras.layers.Resizing(224, 224, interpolation=\"bilinear\", name='resize'),\n",
    "        keras.layers.Rescaling(scale=1./255., name='rescale'),\n",
    "        base_vgg16,\n",
    "        keras.layers.Flatten(name='flatten'),\n",
    "        keras.layers.Dense(1024, activation='relu', name='fully_conn1'),\n",
    "        keras.layers.Dense(512, activation='relu', name='fully_conn2'),\n",
    "        keras.layers.Dense(3, activation='softmax', name='out_softmax'),\n",
    "    ], name = 'vgg16')\n",
    "\n",
    "    vgg16.compile(optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=evaluation.get_training_metrics()\n",
    "                 )\n",
    "    return vgg16\n",
    "    \n",
    "with strategy.scope():\n",
    "    vgg16 = make_vgg16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_VGG16:\n",
    "    vgg16_history, vgg16_time = train_model(vgg16, save=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2 ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_resnet50():\n",
    "    base_resnet50 = tf.keras.applications.ResNet50V2(weights='imagenet', input_shape=(224,224,3), include_top=False)\n",
    "    for layer in base_resnet50.layers:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    resnet50 = tf.keras.Sequential([\n",
    "        keras.layers.InputLayer(input_shape=(512,512,3), name='input'),\n",
    "        keras.layers.Resizing(224, 224, interpolation=\"bilinear\", name='resize'),\n",
    "        keras.layers.Rescaling(scale=1./255., name='rescale'),\n",
    "        base_resnet50,\n",
    "        keras.layers.GlobalAveragePooling2D(name='avg_pool'),\n",
    "        keras.layers.Dense(1024, activation='relu', name='fully_conn1'),\n",
    "        keras.layers.Dense(512, activation='relu', name='fully_conn2'),\n",
    "        keras.layers.Dense(3, activation='softmax', name='out_softmax'),\n",
    "    ], name = 'resnet50')\n",
    "\n",
    "    resnet50.compile(optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=evaluation.get_training_metrics()\n",
    "                 )\n",
    "    return resnet50\n",
    "    \n",
    "with strategy.scope():\n",
    "    resnet50 = make_resnet50()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_RESNET50:\n",
    "    resnet50_history, resnet50_time = train_model(resnet50, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_inception_resnet():\n",
    "    base_inception_resnet = tf.keras.applications.InceptionResNetV2(weights='imagenet', input_shape=(299,299,3), include_top=False)\n",
    "    for layer in base_inception_resnet.layers:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    inception_resnet = tf.keras.Sequential([\n",
    "        keras.layers.InputLayer(input_shape=(512,512,3), name='input'),\n",
    "        keras.layers.Resizing(299, 299, interpolation=\"bilinear\", name='resize'),\n",
    "        keras.layers.Rescaling(scale=1./255., name='rescale'),\n",
    "        base_inception_resnet,\n",
    "        keras.layers.GlobalAveragePooling2D(name='avg_pool'),\n",
    "        keras.layers.Dense(1024, activation='relu', name='fully_conn1'),\n",
    "        keras.layers.Dense(512, activation='relu', name='fully_conn2'),\n",
    "        keras.layers.Dense(3, activation='softmax', name='out_softmax'),\n",
    "    ], name = 'inception_resnet')\n",
    "\n",
    "    inception_resnet.compile(optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=evaluation.get_training_metrics()\n",
    "                 )\n",
    "    return inception_resnet\n",
    "    \n",
    "with strategy.scope():\n",
    "    inception_resnet = make_inception_resnet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_INCEPTION_RESNET:\n",
    "    inception_resnet_history, inception_resnet_time = train_model(inception_resnet, save=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.4 EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_efficientnetb0():\n",
    "    base_efficientnetb0 = tf.keras.applications.EfficientNetV2B0(weights='imagenet', input_shape=(224,224,3), include_top=False)\n",
    "    for layer in base_efficientnetb0.layers:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    efficientnetb0 = tf.keras.Sequential([\n",
    "        keras.layers.InputLayer(input_shape=(512,512,3), name='input'),\n",
    "        keras.layers.Resizing(224, 224, interpolation=\"bilinear\", name='resize'),\n",
    "        base_efficientnetb0,\n",
    "        keras.layers.GlobalAveragePooling2D(name='avg_pool'),\n",
    "        keras.layers.Dense(1024, activation='relu', name='fully_conn1'),\n",
    "        keras.layers.Dense(512, activation='relu', name='fully_conn2'),\n",
    "        keras.layers.Dense(3, activation='softmax', name='out_softmax'),\n",
    "    ], name = 'efficientnetb0')\n",
    "\n",
    "    efficientnetb0.compile(optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=evaluation.get_training_metrics()\n",
    "                 )\n",
    "    return efficientnetb0\n",
    "    \n",
    "with strategy.scope():\n",
    "    efficientnetb0 = make_efficientnetb0()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_EFFICIENTNETB0:\n",
    "    efficientnetb0_history, efficientnetb0_time = train_model(efficientnetb0, save=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.5 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_path = {\n",
    "    'vgg16' : zoidbergManager.model_dir / 'histories' / 'hty_smallds_vgg16.json',\n",
    "    'resnet50' : zoidbergManager.model_dir / 'histories' / 'hty_smallds_resnet50.json',\n",
    "    'inception_resnet' : zoidbergManager.model_dir / 'histories' / 'hty_smallds_inception_resnet.json',\n",
    "    'xception' : zoidbergManager.model_dir / 'histories' / 'hty_smallds_xception.json',\n",
    "    'efficientnetb0' : zoidbergManager.model_dir / 'histories' / 'hty_smallds_efficientnetb0.json'\n",
    "}\n",
    "\n",
    "checkpoint_path = {\n",
    "    'vgg16' : zoidbergManager.model_dir / 'checkpoints' / 'ckpt_smallds_vgg16.h5',\n",
    "    'resnet50' : zoidbergManager.model_dir / 'checkpoints' / 'ckpt_smallds_resnet50.h5',\n",
    "    'inception_resnet' : zoidbergManager.model_dir / 'checkpoints' / 'ckpt_smallds_inception_resnet.h5',\n",
    "    'xception' : zoidbergManager.model_dir / 'checkpoints' / 'ckpt_smallds_xception.h5',\n",
    "    'efficientnetb0' : zoidbergManager.model_dir / 'checkpoints' / 'ckpt_smallds_efficientnetb0.h5'\n",
    "}\n",
    "\n",
    "sumup_result_df = pd.DataFrame(columns=['model','size','training_time','max MCC','max val_MCC'])\n",
    "histories = {}\n",
    "for model_name, path in history_path.items():\n",
    "    with open(path) as file:\n",
    "        history = json.load(file)\n",
    "    histories[model_name] = history['history']\n",
    "    sumup = []\n",
    "    sumup.append(model_name)\n",
    "    sumup.append(f'{checkpoint_path[model_name].stat().st_size / (1e6):.1f} MB')\n",
    "    sumup.append(f\"{history['training_time']:.3f} s\")\n",
    "    sumup.append(f\"{np.max(history['history']['MCC']):.3f}\")\n",
    "    sumup.append(f\"{np.max(history['history']['val_MCC']):.3f}\")\n",
    "    idx_model = list(history_path.keys()).index(model_name)\n",
    "    sumup_result_df.loc[idx_model] = sumup\n",
    "\n",
    "sumup_result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcc_history = {}\n",
    "for model, history in histories.items():\n",
    "    mcc_history[model] = history['val_MCC']\n",
    "\n",
    "sns.set_style('ticks')\n",
    "bp = sns.boxplot(data=pd.DataFrame.from_dict(mcc_history).iloc[-5:])\n",
    "bp.set_title('val_MCC over last 5 epochs')\n",
    "sns.despine(offset=10, trim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = list(range(1,EPOCHS+1))\n",
    "full_mcc_history = []\n",
    "for model, history in histories.items():\n",
    "    data = {}\n",
    "    data['epoch'] = epochs\n",
    "    data['model'] = [model]*EPOCHS\n",
    "    data['MCC'] = history['MCC']\n",
    "    data['val_MCC'] = history['val_MCC']\n",
    "    full_mcc_history.append(pd.DataFrame.from_dict(data))\n",
    "full_mcc_history_df = pd.concat(full_mcc_history)\n",
    "\n",
    "sns.set_style('ticks')\n",
    "rp = sns.relplot(\n",
    "    data=pd.melt(full_mcc_history_df,['epoch','model']),\n",
    "    x=\"epoch\",\n",
    "    y=\"value\",\n",
    "    hue=\"variable\",\n",
    "    col=\"model\",\n",
    "    col_wrap=2,\n",
    "    height=2.5,\n",
    "    aspect=2.2,\n",
    "    kind=\"line\"\n",
    ")\n",
    "rp.fig.suptitle('Learning curves', fontsize=14)\n",
    "rp.fig.subplots_adjust(top=0.9)\n",
    "sns.move_legend(rp,\n",
    "                'center',\n",
    "                bbox_to_anchor=(.7, .18),\n",
    "                fontsize=14,\n",
    "                title_fontsize=14,\n",
    "                frameon=True,\n",
    "                borderpad=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
